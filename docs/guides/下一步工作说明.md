# 题目去重功能 - 下一步工作说明

## 📊 当前进度

### ✅ 已完成
- 数据分组功能
- 进度管理和断点续传
- **步骤1：清洗题干**
- **步骤2：秒筛完全一样的题**

### 🔨 下一步：实现步骤3 - 提取特征片段（N-gram）

## 🎯 步骤3：提取特征片段（N-gram）

### 目标
将题目内容切分成小的特征片段，作为计算重复的基础。

### 需要实现的功能

在 `src/services/question_dedup_service.py` 中添加：

```python
@staticmethod
def _extract_ngrams(content: str, n: int = 3) -> set:
    """
    提取 N-gram 特征片段
    
    Args:
        content: 题目内容（已清洗）
        n: n-gram 大小（默认3，中文推荐3-gram）
    
    Returns:
        n-gram 集合（使用set去重）
    """
    # 1. 按顺序切出所有 n 个字符的小片段
    # 2. 使用集合去重（同一题目内的重复片段只算一次）
    # 3. 短题目特殊处理（<3字符使用完整文本或2-gram）
    pass
```

### 实现要点

1. **N-gram 提取**：
   - 例如："如何比较题库" → `["如何比", "何比较", "较题库"]`
   - 使用滑动窗口，每次移动一个字符

2. **去重处理**：
   - 同一题目内的重复片段只算一次（使用 `set`）

3. **短题目处理**：
   - 题目长度 < 3 字符：使用完整文本或降级到 2-gram
   - 题目长度 = 3 字符：直接作为唯一特征

4. **返回值**：
   - 返回 `set` 类型，便于后续计算交集和并集

### 参考文档
- `docs/api/题目去重方案.md` 第 135-163 行

### 测试建议

创建测试用例：
```python
# 测试用例
assert _extract_ngrams("如何比较题库", 3) == {"如何比", "何比较", "较题库"}
assert _extract_ngrams("ABC", 3) == {"ABC"}  # 短题目
assert _extract_ngrams("AB", 3) == {"AB"}    # 超短题目
```

### 在 process_single_group() 中的使用

步骤3需要在步骤2之后执行，只对非完全重复的题目进行N-gram提取：

```python
# 步骤2之后，筛选出非完全重复的题目
non_duplicate_question_ids = set()
for q in cleaned_questions:
    non_duplicate_question_ids.add(q['question_id'])
for dup in exact_duplicates:
    for qid in dup['question_ids']:
        non_duplicate_question_ids.discard(qid)

# 步骤3 - 提取特征片段
ngram_features = {}
for item in cleaned_questions:
    if item['question_id'] in non_duplicate_question_ids:
        ngrams = QuestionDedupService._extract_ngrams(item['cleaned_content'])
        ngram_features[item['question_id']] = ngrams
```

---

## 📝 后续步骤预览

实现步骤3后，还需要：

1. **步骤4：生成指纹（MinHash）** - 将n-gram集合压缩成128位指纹向量
2. **步骤5：LSH分桶** - 将可能重复的题目分到同一个桶中
3. **步骤6：桶内精算** - 计算Jaccard相似度，找出相似重复的题目对

---

**最后更新**：2026-01-03

