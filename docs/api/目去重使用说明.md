# 题目去重功能使用说明

## 一、功能概述

题目去重服务已实现单分组处理、进度记录和断点续传功能。当前已完成的模块：

### ✅ 已完成

1. **数据分组功能** (`QuestionService.get_question_groups()`)

   - 按 `(type, subject_id, channel_code)` 三元组分组
   - 统计每组题目数量
   - 支持查询分组列表

2. **进度管理功能** (`QuestionDedupService`)

   - 单分组顺序处理
   - 进度文件记录（`question_dedup_progress.json`）
   - 支持中断后继续处理
   - 处理状态跟踪

3. **步骤 1：清洗题干** ✅

   - 去除 HTML 标签
   - 全角转半角
   - 空格标准化
   - 图片/公式占位符标准化
   - 去除不可见字符

4. **步骤 2：秒筛完全一样的题** ✅
   - 使用 MD5 哈希值分组
   - 快速找出内容完全相同的题目
   - 返回完全重复的题目组列表

### 🔨 待实现

以下算法步骤需要在 `QuestionDedupService.process_single_group()` 方法中实现：

1. **步骤 3：提取特征片段（N-gram）**
2. **步骤 4：生成指纹（MinHash）**
3. **步骤 5：LSH 分桶**
4. **步骤 6：桶内精算重复程度**

---

## 二、当前使用方式

### 2.1 初始化去重会话

```python
from src.app import app, db
from src.services.question_dedup_service import QuestionDedupService

with app.app_context():
    # 初始化（会获取所有分组）
    progress = QuestionDedupService.init_dedup_session()
    print(f"总分组数: {progress['total_groups']}")
```

### 2.2 处理单个分组

```python
# 方法1：使用便捷方法（推荐）
results = QuestionDedupService.process_next_group()
if results:
    print(f"处理完成: {results['group']['type_name']}")
else:
    print("所有分组都已处理完成！")

# 方法2：手动控制
group = QuestionDedupService.get_next_group()
if group:
    results = QuestionDedupService.process_single_group(group)
    QuestionDedupService.mark_group_completed(results)
```

### 2.3 查看进度

```python
summary = QuestionDedupService.get_summary()
print(f"状态: {summary['status']}")
print(f"进度: {summary['processed_groups']}/{summary['total_groups']}")
print(f"完成度: {summary['progress_percentage']:.2f}%")
```

### 2.4 断点续传

如果程序中断，重新运行时会自动从上次中断的位置继续：

```python
# 直接调用即可，会自动从上次位置继续
results = QuestionDedupService.process_next_group()
```

### 2.5 重置进度

如果需要重新开始：

```python
progress = QuestionDedupService.reset_progress()
```

---

## 三、进度文件说明

进度文件位置：`question_dedup_progress.json`（项目根目录）

文件结构：

```json
{
  "current_group_index": 3,          // 当前处理的分组索引（从0开始）
  "total_groups": 537,                // 总分组数
  "processed_groups": 3,              // 已处理的分组数
  "current_group": null,              // 当前分组信息（处理中时有值）
  "status": "pending",                // 状态：pending/running/completed/error
  "last_update": "2026-01-03T13:55:52.515494",
  "groups": [...],                    // 所有分组列表
  "results": [...]                    // 处理结果（可选）
}
```

---

## 四、接下来要实现的步骤

### 步骤 1：清洗题干 ✅

**状态**：已完成

**实现位置**：`QuestionDedupService._clean_question_content()` 和 `_clean_questions()`

**功能**：

- ✅ 去除 HTML 标签
- ✅ 全角转半角转换
- ✅ 空格标准化（多个空格合并为一个，去除首尾空白）
- ✅ 图片/公式占位符标准化：`[图片1]` → `[IMG]`, `[公式1]` → `[FORMULA]`
- ✅ 去除不可见字符

**测试脚本**：`scripts/test/test_dedup_steps_1_2.py`

---

### 步骤 2：秒筛完全一样的题 ✅

**状态**：已完成

**实现位置**：`QuestionDedupService._find_exact_duplicates()`

**功能**：

- ✅ 使用 MD5 哈希值快速分组
- ✅ 找出内容完全相同的题目
- ✅ 返回完全重复的题目组列表（包含题目 ID 和相似度 1.0）

**性能**：对于 1000 题的分组，处理时间 < 1 秒

**测试脚本**：`scripts/test/test_dedup_steps_1_2.py`

---

### 步骤 3：提取特征片段（N-gram）

**目标**：将题目内容切分成小的特征片段

**实现位置**：在步骤 2 之后

**需要实现的功能**：

```python
@staticmethod
def _extract_ngrams(content: str, n: int = 3) -> set:
    """
    提取 N-gram 特征片段

    Args:
        content: 题目内容
        n: n-gram 大小（默认3）

    Returns:
        n-gram 集合
    """
    # 1. 按顺序切出所有 n 个字符的小片段
    # 2. 使用集合去重（同一题目内的重复片段只算一次）
    # 3. 短题目特殊处理（<3字符使用完整文本或2-gram）
    pass
```

**参考文档**：`docs/api/题目去重方案.md` 第 135-163 行

---

### 步骤 4：生成指纹（MinHash）

**目标**：将 n-gram 集合压缩成固定长度的指纹向量

**需要安装的库**：

```bash
pip install datasketch
```

或者手动实现 MinHash 算法。

**实现位置**：在步骤 3 之后

**需要实现的功能**：

```python
@staticmethod
def _generate_minhash(ngrams: set, num_perm: int = 128) -> List[int]:
    """
    生成 MinHash 指纹

    Args:
        ngrams: n-gram 集合
        num_perm: 哈希函数数量（默认128）

    Returns:
        指纹向量（128个整数）
    """
    # 1. 使用 128 个不同的哈希函数（通过种子区分）
    # 2. 对每个哈希函数，计算所有 n-gram 的哈希值
    # 3. 取每个哈希函数的最小哈希值
    # 4. 返回 128 位指纹向量
    pass
```

**参考文档**：`docs/api/题目去重方案.md` 第 166-195 行

---

### 步骤 5：LSH 分桶

**目标**：将可能重复的题目分到同一个桶中

**实现位置**：在步骤 4 之后

**需要实现的功能**：

```python
@staticmethod
def _lsh_bucketing(fingerprints: Dict[int, List[int]],
                   num_bands: int = 16,
                   rows_per_band: int = 8) -> Dict[str, List[int]]:
    """
    LSH 分桶

    Args:
        fingerprints: 题目ID到指纹向量的映射
        num_bands: Band 数量（默认16）
        rows_per_band: 每个 Band 的行数（默认8）

    Returns:
        桶ID到题目ID列表的映射
    """
    # 1. 将 128 位指纹切成 16 段（每段 8 位）
    # 2. 对每一段计算哈希值，得到 16 个桶 ID
    # 3. 每道题可能出现在 16 个不同的桶中
    # 4. 返回桶ID到题目列表的映射
    # 5. 只处理桶内题目数量>1的桶
    pass
```

**参考文档**：`docs/api/题目去重方案.md` 第 198-229 行

---

### 步骤 6：桶内精算重复程度

**目标**：在桶内精确计算每对题目的相似度

**实现位置**：在步骤 5 之后

**需要实现的功能**：

```python
@staticmethod
def _calculate_similarity(buckets: Dict[str, List[int]],
                          ngram_features: Dict[int, set],
                          threshold: float = 0.8) -> List[Dict[str, Any]]:
    """
    计算桶内题目的相似度

    Args:
        buckets: LSH 分桶结果
        ngram_features: 题目ID到ngram集合的映射
        threshold: 相似度阈值（默认0.8）

    Returns:
        重复题目对列表
        [
            {
                'question_id_1': 123,
                'question_id_2': 456,
                'similarity': 0.95
            },
            ...
        ]
    """
    # 1. 对桶内每对题目计算 Jaccard 相似度
    #   相似度 = 交集大小 / 并集大小
    # 2. 只输出相似度 >= 阈值的结果
    # 3. 去重（同一对可能出现在多个桶中）
    pass
```

**参考文档**：`docs/api/题目去重方案.md` 第 232-272 行

---

## 五、实现顺序建议

推荐按以下顺序实现：

1. ✅ **数据分组**（已完成）
2. ✅ **进度管理**（已完成）
3. ✅ **步骤 1：清洗题干**（已完成）
4. ✅ **步骤 2：秒筛完全一样的题**（已完成）
5. 🔨 **步骤 3：提取特征片段**（N-gram，相对简单）- **下一步**
6. 🔨 **步骤 4：生成指纹**（MinHash，需要算法实现）
7. 🔨 **步骤 5：LSH 分桶**（需要理解 LSH 原理）
8. 🔨 **步骤 6：桶内精算**（Jaccard 相似度计算）

---

## 六、测试建议

每实现一个步骤，建议：

1. 在 `process_single_group()` 中逐步添加代码
2. 使用单个小分组测试（题目数<100）
3. 验证输出结果是否正确
4. 检查性能是否满足要求

测试脚本示例：

```python
from src.app import app, db
from src.services.question_dedup_service import QuestionDedupService

with app.app_context():
    # 处理一个分组并查看结果
    results = QuestionDedupService.process_next_group()
    if results:
        print(f"完全重复: {len(results['exact_duplicates'])} 组")
        print(f"相似重复: {len(results['similar_duplicates'])} 对")
```

---

## 七、性能目标

- **单个分组处理时间**：根据题目数量，一般 < 10 秒（1000 题以内）
- **内存占用**：单分组处理，内存占用可控
- **总处理时间**：537 个分组，预计 1-2 小时（取决于题目数量）

---

## 八、注意事项

1. **进度文件安全**：进度文件在项目根目录，建议加入 `.gitignore`
2. **错误处理**：如果某个分组处理失败，会记录错误状态，可以手动修复后继续
3. **结果保存**：处理结果保存在进度文件中，可以后续导出
4. **资源管理**：处理大分组时注意内存使用，可以分批处理

---

**最后更新**：2026-01-03
